# -*- coding: utf-8 -*-
"""
This script converts GROBID-processed TEI XML files into structured Markdown files.

The script extracts key metadata (title, authors, DOI, year) from the XML and
formats it as YAML front matter. It then parses the main body of the XML,
converting headings and paragraphs into Markdown. The output is a clean,
readable Markdown file suitable for ingestion into a vector database or for
static site generators.

Key functionalities include:
- Parsing TEI XML files to extract structured data.
- Generating YAML front matter for metadata.
- Converting the XML body into Markdown, preserving section hierarchy.
- Handling missing or malformed data gracefully.
- Removing irrelevant sections like references and acknowledgements.
"""

import os
import re
from bs4 import BeautifulSoup


def xml_to_markdown(xml_file_path, output_dir):
    """Converts a GROBID TEI XML file to a structured Markdown file.

    This function parses a TEI XML file generated by GROBID, extracts metadata
    to create a YAML front matter block, and converts the main text content
    into Markdown format. It intelligently handles section headings and paragraph
    content, while filtering out sections that are not useful for semantic
    analysis (e.g., references, acknowledgements).

    Args:
        xml_file_path (str): The full path to the input XML file.
        output_dir (str): The directory where the resulting Markdown file
                          will be saved.
    """
    try:
        with open(xml_file_path, 'r', encoding='utf-8') as f:
            soup = BeautifulSoup(f, 'lxml-xml')

        # --- Pre-process to remove unwanted sections ---
        unwanted_section_headings = [
            "references", "bibliography", "acknowledgement", "acknowledgements",
            "declaration of competing interest", "credit authorship contribution statement"
        ]
        for div in soup.find_all('div'):
            head = div.find('head')
            if head and head.get_text(strip=True).lower() in unwanted_section_headings:
                div.decompose()

        # --- 1. Extract Metadata (with safety checks) ---
        
        title = "No Title Found"
        authors = []
        doi = ""
        year = ""
        abstract_text = ""

        # Safely find the title
        title_stmt = soup.find('titleStmt')
        if title_stmt:
            title_tag = title_stmt.find('title')
            if title_tag:
                title = title_tag.get_text(strip=True)
        # Fallback: If no titleStmt, try to get title from the first <p> after the first <head>
        else:
            body = soup.find('body')
            if body and body.find_all('div'):
                # The title is often in the first <p> tag of the first <div>
                first_p = body.find_all('div')[0].find('p')
                if first_p:
                    title = first_p.get_text(strip=True)

        title = title.replace('"', '\\"')

        # Safely find authors
        analytic_section = soup.find('analytic')
        if analytic_section:
            for author in analytic_section.find_all('author'):
                pers_name = author.find('persName')
                if pers_name:
                    forenames = " ".join([fn.get_text(strip=True) for fn in pers_name.find_all('forename')])
                    surname_tag = pers_name.find('surname')
                    surname = surname_tag.get_text(strip=True) if surname_tag else ""
                    authors.append(f"{forenames} {surname}".strip())
        
        # Safely find DOI
        doi_tag = soup.find('idno', type='DOI')
        if doi_tag:
            doi = doi_tag.get_text(strip=True)
            
        # Safely find Abstract
        abstract_tag = soup.find('abstract')
        if abstract_tag:
            abstract_text = "\n".join([p.get_text(strip=True) for p in abstract_tag.find_all('p')])

        # Safely find Year
        date_source = soup.find('publicationStmt') or soup.find('monogr')
        if date_source:
            date_tag = date_source.find('date')
            if date_tag:
                if date_tag.get('when'):
                    year = date_tag['when'][:4]
                else:
                    year_match = re.search(r'\b\d{4}\b', date_tag.get_text(strip=True))
                    if year_match:
                        year = year_match.group(0)

        # --- 2. Create YAML Front Matter ---
        yaml_front_matter = "---\n"
        yaml_front_matter += f'title: "{title}"\n'
        yaml_front_matter += "authors:\n"
        if authors:
            for author in authors:
                yaml_front_matter += f'  - "{author}"\n'
        else:
            yaml_front_matter += "  - Not Available\n" # Add placeholder if no authors found
        yaml_front_matter += f'doi: "{doi}"\n'
        yaml_front_matter += f'year: "{year}"\n'
        yaml_front_matter += "---\n\n"

        # --- 3. Process the Body Content ---
        markdown_body = []
        
        if abstract_text:
            markdown_body.append("## Abstract")
            markdown_body.append(abstract_text)

        body_tag = soup.find('body')
        if body_tag:
            for section in body_tag.find_all('div', recursive=False):
                heading_tag = section.find('head')
                if heading_tag:
                    heading_text = heading_tag.get_text(strip=True)
                    n_attr = heading_tag.get('n', '')
                    heading_level = n_attr.count('.') + 2 if n_attr else 2
                    heading_prefix = '#' * heading_level
                    markdown_body.append(f"\n{heading_prefix} {heading_text}")
                    
                # Process paragraphs directly under the div, not just after a head
                for p in section.find_all('p', recursive=False):
                    markdown_body.append(p.get_text(strip=True))

        # --- 4. Assemble and Save the Markdown File ---
        final_markdown = yaml_front_matter + "\n\n".join(markdown_body)
        os.makedirs(output_dir, exist_ok=True)

        base_name = os.path.basename(xml_file_path)
        if base_name.endswith('.grobid.tei.xml'):
            file_name_without_ext = base_name[:-len('.grobid.tei.xml')]
        else:
            file_name_without_ext = os.path.splitext(base_name)[0]
            
        output_path = os.path.join(output_dir, f"{file_name_without_ext}.md")

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(final_markdown)
            
        print(f"Successfully converted '{xml_file_path}' to '{output_path}'")

    except Exception as e:
        print(f"Error processing '{xml_file_path}': {e}")

# --- Example Usage ---
if __name__ == '__main__':
    input_dir = r"C:\Users\td00654\Documents\EDRC LLM Project\CREDS Papers\CREDS - HTML"
    output_dir = r'C:\Users\td00654\Documents\EDRC LLM Project\CREDS Papers\CREDS - Full'
    
    for filename in os.listdir(input_dir):
        if filename.endswith('.xml'):
            xml_to_markdown(os.path.join(input_dir, filename), output_dir)